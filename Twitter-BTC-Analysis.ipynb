{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import pandasql as ps\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0,1,2,3,4,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Import Tweets - https://www.kaggle.com/alaix14/bitcoin-tweets-20160101-to-20190329\n",
    "#tweets_df = pd.read_csv('tweets.csv', error_bad_lines=False, \n",
    "#                        names=['user', 'fullname', 'timestamp', 'replies', 'likes', 'retweets', 'text'], \n",
    "#                        sep=';', header=2, nrows=10)\n",
    "tweets_df = pd.read_csv('tweets.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop url, id, fullname\n",
    "tweets_df = tweets_df.drop('url', 1)\n",
    "tweets_df = tweets_df.drop('id', 1)\n",
    "tweets_df = tweets_df.drop('fullname', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where text is null\n",
    "tweets_df = tweets_df.dropna(axis=0, subset=['text'])\n",
    "# Drop all rows where timestamp is null\n",
    "tweets_df = tweets_df.dropna(axis=0, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change timestamp dtype\n",
    "tweets_df['timestamp'] = tweets_df['timestamp'].apply(lambda x: datetime.strptime(x[:-3], '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bad_patterns(text):\n",
    "    \"\"\"Remove html, latex, and newline characters from a string\n",
    "    \n",
    "    :param text: content as a string\n",
    "    :return: cleaned text string\n",
    "    \"\"\"\n",
    "    html_tag = re.compile('<.*?>')\n",
    "    stripped_html = re.sub(html_tag, '', text)\n",
    "    stripped_n = stripped_html.strip().replace('\\n', '')\n",
    "    return stripped_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the text data - remove tags/new lines\n",
    "tweets_df['text'] = tweets_df['text'].apply(lambda x: remove_bad_patterns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dfs by time\n",
    "# March 22, 2017 ($1036.10) - June 10, 2017 ($2910.34): First big rise - period_one\n",
    "# June 10, 2017 ($2910.34) - July 14, 2017 ($2109.37): First big fall - period_two\n",
    "# Sept 14, 2017 ($3358.11) - Dec 16, 2017 ($19166.88): Massive peak in 2017 - period_three\n",
    "# Dec 16, 2017 ($19166.88) - Feb 5, 2018 ($6332.37): Massive fall in 2017/18 - period_four\n",
    "# Feb 5, 2018 ($6332.37) - Dec 15, 2018 ($3194.96): Intense volatility and uncertainty - period_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_one = tweets_df[(tweets_df['timestamp'] > '2017-03-22 00:00:00') & (tweets_df['timestamp'] < '2017-06-10 23:59:00')]\n",
    "period_two = tweets_df[(tweets_df['timestamp'] > '2017-06-10 00:00:00') & (tweets_df['timestamp'] < '2017-07-14 23:59:00')]\n",
    "period_three = tweets_df[(tweets_df['timestamp'] > '2017-07-14 00:00:00') & (tweets_df['timestamp'] < '2017-12-16 23:59:00')]\n",
    "period_four = tweets_df[(tweets_df['timestamp'] > '2017-12-16 00:00:00') & (tweets_df['timestamp'] < '2018-02-05 23:59:00')]\n",
    "period_five = tweets_df[(tweets_df['timestamp'] > '2018-02-05 00:00:00') & (tweets_df['timestamp'] < '2018-12-15 23:59:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31333\n",
      "50264\n",
      "783011\n",
      "76104\n",
      "2107050\n"
     ]
    }
   ],
   "source": [
    "print(len(period_one))\n",
    "print(len(period_two))\n",
    "print(len(period_three))\n",
    "print(len(period_four))\n",
    "print(len(period_five))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use tweets that have at least 10 likes, aka delete the losers\n",
    "period_one_cleaned = period_one[(period_one['likes'] >= 10)]\n",
    "period_two_cleaned = period_two[(period_two['likes'] >= 10)]\n",
    "period_three_cleaned = period_three[(period_three['likes'] >= 10)]\n",
    "period_four_cleaned = period_four[(period_four['likes'] >= 10)]\n",
    "period_five_cleaned = period_five[(period_five['likes'] >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n",
      "872\n",
      "12643\n",
      "2135\n",
      "37777\n"
     ]
    }
   ],
   "source": [
    "print(len(period_one_cleaned))\n",
    "print(len(period_two_cleaned))\n",
    "print(len(period_three_cleaned))\n",
    "print(len(period_four_cleaned))\n",
    "print(len(period_five_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011138416366131554\n",
      "0.017348400445646985\n",
      "0.016146644172304093\n",
      "0.028053715967623252\n",
      "0.017928857881872762\n"
     ]
    }
   ],
   "source": [
    "print(len(period_one_cleaned)/len(period_one))\n",
    "print(len(period_two_cleaned)/len(period_two))\n",
    "print(len(period_three_cleaned)/len(period_three))\n",
    "print(len(period_four_cleaned)/len(period_four))\n",
    "print(len(period_five_cleaned)/len(period_five))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_one_text = period_one_cleaned['text'].tolist()\n",
    "period_two_text = period_two_cleaned['text'].tolist()\n",
    "period_three_text = period_three_cleaned['text'].tolist()\n",
    "period_four_text = period_four_cleaned['text'].tolist()\n",
    "period_five_text = period_five_cleaned['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunramakrishnan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varunramakrishnan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bitcoin specific words\n",
    "stopwords.add('bitcoin')\n",
    "stopwords.add('btc')\n",
    "stopwords.add('http')\n",
    "stopwords.add('https')\n",
    "stopwords.add('r')\n",
    "stopwords.add('www')\n",
    "stopwords.add('.com')\n",
    "stopwords.add('...')\n",
    "stopwords.add('…')\n",
    "stopwords.add('’')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize_content(content):\n",
    "  \"\"\"returns tokenized string\n",
    "\n",
    "  :param content: text string\n",
    "  :return: tokenized text/list of words\n",
    "  \"\"\"\n",
    "  list_of_digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "  # Create tokens, make all lowercase, remove all stopwords\n",
    "  tokens = nltk.word_tokenize(content)\n",
    "  tokens = [token.lower() for token in tokens]\n",
    "  tokens_cleaned = [token for token in tokens if token not in stopwords and token not in string.punctuation\n",
    "                    and token[0] not in string.punctuation]\n",
    "\n",
    "  # Extra operations for word cloud - remove numbers\n",
    "  tokens_no_numbers = [token for token in tokens_cleaned if not any(c.isdigit() for c in token)]\n",
    "  # Take out punctuation in the middle\n",
    "  final_tokens = [token for token in tokens_no_numbers if not any(p in token for p in string.punctuation)]\n",
    "  return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_highest = [tokenize_content(item) for item in period_one_text]\n",
    "highest_tokens = [item for sublist in all_highest for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "highest_counter = Counter(highest_tokens) \n",
    "highest_most_common_all = highest_counter.most_common\n",
    "highest_most_common = highest_counter.most_common(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blockchain', 52),\n",
       " ('price', 26),\n",
       " ('usd', 24),\n",
       " ('crypto', 21),\n",
       " ('amp', 21),\n",
       " ('new', 17),\n",
       " ('fintech', 17),\n",
       " ('may', 16),\n",
       " ('latest', 15),\n",
       " ('ico', 14),\n",
       " ('today', 14),\n",
       " ('eth', 14),\n",
       " ('mining', 14),\n",
       " ('time', 13),\n",
       " ('ethereum', 13),\n",
       " ('gmt', 13),\n",
       " ('pm', 12),\n",
       " ('buy', 12),\n",
       " ('april', 12),\n",
       " ('index', 12)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/varunramakrishnan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis using VADER (given social medida focus)\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period 1 through 5 Sentiment Scores\n",
    "period_one_cleaned['neg'] = period_one_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neg'], axis=1)\n",
    "period_one_cleaned['neu'] = period_one_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neu'], axis=1)\n",
    "period_one_cleaned['pos'] = period_one_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['pos'], axis=1)\n",
    "period_one_cleaned['compound'] = period_one_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)\n",
    "\n",
    "period_two_cleaned['neg'] = period_two_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neg'], axis=1)\n",
    "period_two_cleaned['neu'] = period_two_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neu'], axis=1)\n",
    "period_two_cleaned['pos'] = period_two_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['pos'], axis=1)\n",
    "period_two_cleaned['compound'] = period_two_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)\n",
    "\n",
    "period_three_cleaned['neg'] = period_three_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neg'], axis=1)\n",
    "period_three_cleaned['neu'] = period_three_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neu'], axis=1)\n",
    "period_three_cleaned['pos'] = period_three_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['pos'], axis=1)\n",
    "period_three_cleaned['compound'] = period_three_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)\n",
    "\n",
    "period_four_cleaned['neg'] = period_four_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neg'], axis=1)\n",
    "period_four_cleaned['neu'] = period_four_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neu'], axis=1)\n",
    "period_four_cleaned['pos'] = period_four_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['pos'], axis=1)\n",
    "period_four_cleaned['compound'] = period_four_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_five_cleaned['neg'] = period_five_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neg'], axis=1)\n",
    "period_five_cleaned['neu'] = period_five_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['neu'], axis=1)\n",
    "period_five_cleaned['pos'] = period_five_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['pos'], axis=1)\n",
    "period_five_cleaned['compound'] = period_five_cleaned.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_one_neg = period_one_cleaned['neg'].mean()\n",
    "period_one_neu = period_one_cleaned['neu'].mean()\n",
    "period_one_pos = period_one_cleaned['pos'].mean()\n",
    "period_one_compound = period_one_cleaned['compound'].mean()\n",
    "\n",
    "period_two_neg = period_two_cleaned['neg'].mean()\n",
    "period_two_neu = period_two_cleaned['neu'].mean()\n",
    "period_two_pos = period_two_cleaned['pos'].mean()\n",
    "period_two_compound = period_two_cleaned['compound'].mean()\n",
    "\n",
    "period_three_neg = period_three_cleaned['neg'].mean()\n",
    "period_three_neu = period_three_cleaned['neu'].mean()\n",
    "period_three_pos = period_three_cleaned['pos'].mean()\n",
    "period_three_compound = period_three_cleaned['compound'].mean()\n",
    "\n",
    "period_four_neg = period_four_cleaned['neg'].mean()\n",
    "period_four_neu = period_four_cleaned['neu'].mean()\n",
    "period_four_pos = period_four_cleaned['pos'].mean()\n",
    "period_four_compound = period_four_cleaned['compound'].mean()\n",
    "\n",
    "period_five_neg = period_five_cleaned['neg'].mean()\n",
    "period_five_neu = period_five_cleaned['neu'].mean()\n",
    "period_five_pos = period_five_cleaned['pos'].mean()\n",
    "period_five_compound = period_five_cleaned['compound'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period One\n",
      "0.029315186246418337\n",
      "0.8873237822349573\n",
      "0.08335530085959883\n",
      "0.12711232091690544\n",
      "Period Two\n",
      "0.04455045871559632\n",
      "0.887211009174312\n",
      "0.06824082568807344\n",
      "0.05573463302752292\n",
      "Period Three\n",
      "0.040174958475045465\n",
      "0.8857470537056052\n",
      "0.07399564976666936\n",
      "0.09981891165071592\n",
      "Period Four\n",
      "0.030654332552693196\n",
      "0.8988562060889944\n",
      "0.07049227166276352\n",
      "0.1333743325526934\n",
      "Period Five\n",
      "0.03279005744235934\n",
      "0.8880077295709025\n",
      "0.07920173650633962\n",
      "0.1479023718135382\n"
     ]
    }
   ],
   "source": [
    "print(\"Period One\")\n",
    "print(period_one_neg)\n",
    "print(period_one_neu)\n",
    "print(period_one_pos)\n",
    "print(period_one_compound)\n",
    "\n",
    "print(\"Period Two\")\n",
    "print(period_two_neg)\n",
    "print(period_two_neu)\n",
    "print(period_two_pos)\n",
    "print(period_two_compound)\n",
    "\n",
    "print(\"Period Three\")\n",
    "print(period_three_neg)\n",
    "print(period_three_neu)\n",
    "print(period_three_pos)\n",
    "print(period_three_compound)\n",
    "\n",
    "print(\"Period Four\")\n",
    "print(period_four_neg)\n",
    "print(period_four_neu)\n",
    "print(period_four_pos)\n",
    "print(period_four_compound)\n",
    "\n",
    "print(\"Period Five\")\n",
    "print(period_five_neg)\n",
    "print(period_five_neu)\n",
    "print(period_five_pos)\n",
    "print(period_five_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using FastText\n",
    "import fasttext\n",
    "# Omit because data labelling/supervised learning is not appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master sentiment CSV to compare sentiment/Twitter activity with price\n",
    "# Start by loading Bitcoin price data\n",
    "prices_df = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime format for join\n",
    "prices_df['Timestamp'] = prices_df['Timestamp'].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Change type of column to datetime\n",
    "prices_df['Timestamp']= pd.to_datetime(prices_df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically, we have to calculate sentiment for everything in the previous dataframe\n",
    "# Also, do group bys to organize data by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate datetime to hour\n",
    "tweets_df['timestamp'] = tweets_df['timestamp'].apply(lambda x: x.replace(microsecond=0, second=0, minute=0))\n",
    "# Calculate compound score\n",
    "tweets_df['Compound_Score'] = tweets_df.apply(lambda x: sid.polarity_scores(x['text'])['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "tweets_df = tweets_df.drop('replies', 1)\n",
    "tweets_df = tweets_df.drop('likes', 1)\n",
    "tweets_df = tweets_df.drop('retweets', 1)\n",
    "tweets_df = tweets_df.drop('text', 1)\n",
    "tweets_df = tweets_df.drop('user', 1)\n",
    "\n",
    "# Add number of Tweets\n",
    "tweets_df['Total_Num_Tweets'] = tweets_df.groupby(['timestamp'])['Compound_Score'].transform('count')\n",
    "\n",
    "# Group by timestamp \n",
    "tweets_df = tweets_df.groupby(by=[\"timestamp\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two dataframes on timestamp!\n",
    "# Left join on price \n",
    "sentiments_df = prices_df.merge(tweets_df, left_on='Timestamp', right_on='timestamp', how='left')\n",
    "# Drop timestamp\n",
    "sentiments_df = sentiments_df.drop('timestamp', 1)\n",
    "sentiments_df = sentiments_df.drop('Weighted_Price', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of negative, positive, and neutral tweets using sql\n",
    "query = '''SELECT Timestamp, Open, High, Low, Close, [Volume (BTC)], [Volume (Currency)], Compound_Score, Total_Num_Tweets\n",
    ",(CASE WHEN Compound_Score < 0 THEN 1 ELSE 0 END) AS Num_Negatives\n",
    ",(CASE WHEN Compound_Score > 0 THEN 1 ELSE 0 END) AS Num_Positives\n",
    ",(CASE WHEN Compound_Score = 0 THEN 1 ELSE 0 END) AS Num_Neutrals\n",
    "FROM sentiments_df\n",
    "'''\n",
    "\n",
    "output_df = ps.sqldf(query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as sentiments.csv\n",
    "output_df.to_csv('sentiments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
